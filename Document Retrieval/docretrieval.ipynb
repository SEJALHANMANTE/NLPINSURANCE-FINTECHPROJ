{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dab9c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Using cached pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting python-docx\n",
      "  Using cached python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Using cached pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Using cached pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six==20250327->pdfplumber)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20250327->pdfplumber)\n",
      "  Downloading cryptography-44.0.2-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Downloading lxml-5.3.2-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from python-docx) (4.13.2)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
      "  Using cached langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.3.30-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.16-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.1-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "Using cached pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "Using cached python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 372.9 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 508.5 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 508.5 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 613.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 758.1 kB/s eta 0:00:00\n",
      "Using cached langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.3.30-py3-none-any.whl (358 kB)\n",
      "Downloading lxml-5.3.2-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.3/3.8 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.4/3.8 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.6/3.8 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading pillow-11.2.1-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 2.8 MB/s eta 0:00:00\n",
      "Using cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 3.3 MB/s eta 0:00:00\n",
      "Using cached pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.1 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.5 MB/s eta 0:00:00\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Downloading cryptography-44.0.2-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.8/3.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.1/3.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 3.8 MB/s eta 0:00:00\n",
      "Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached orjson-3.10.16-cp312-cp312-win_amd64.whl (133 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl (495 kB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: zstandard, urllib3, typing-inspection, tqdm, tenacity, sniffio, regex, PyYAML, pypdfium2, pydantic-core, pycparser, Pillow, orjson, lxml, jsonpointer, joblib, idna, h11, greenlet, click, charset-normalizer, certifi, annotated-types, SQLAlchemy, requests, python-docx, pydantic, nltk, jsonpatch, httpcore, cffi, anyio, requests-toolbelt, httpx, cryptography, pdfminer.six, langsmith, pdfplumber, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed Pillow-11.2.1 PyYAML-6.0.2 SQLAlchemy-2.0.40 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.1 click-8.1.8 cryptography-44.0.2 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 idna-3.10 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.23 langchain-core-0.3.51 langchain-text-splitters-0.3.8 langsmith-0.3.30 lxml-5.3.2 nltk-3.9.1 orjson-3.10.16 pdfminer.six-20250327 pdfplumber-0.11.6 pycparser-2.22 pydantic-2.11.3 pydantic-core-2.33.1 pypdfium2-4.30.1 python-docx-1.1.2 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tqdm-4.67.1 typing-inspection-0.4.0 urllib3-2.4.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber python-docx nltk langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5525cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sejal\n",
      "[nltk_data]     Hanmante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c421df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.6-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.14 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.14-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: colorama in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.6-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.14-py3-none-any.whl (213 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.6 jupyterlab_widgets-3.0.14 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5638e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sejal\n",
      "[nltk_data]     Hanmante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from ipywidgets import FileUpload\n",
    "import io\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0716cb",
   "metadata": {},
   "source": [
    "###  Text Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717b9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_stream):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_stream) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file_stream):\n",
    "    doc = docx.Document(file_stream)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_txt(file_stream):\n",
    "    return file_stream.read().decode('utf-8')\n",
    "\n",
    "def extract_text(uploaded_file):\n",
    "    filename = list(uploaded_file.value.keys())[0]\n",
    "    content = uploaded_file.value[filename]['content']\n",
    "    ext = filename.split('.')[-1].lower()\n",
    "    file_stream = io.BytesIO(content)\n",
    "\n",
    "    if ext == 'pdf':\n",
    "        return extract_text_from_pdf(file_stream)\n",
    "    elif ext == 'docx':\n",
    "        return extract_text_from_docx(file_stream)\n",
    "    elif ext == 'txt':\n",
    "        return extract_text_from_txt(file_stream)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please upload a PDF, DOCX, or TXT file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfae30",
   "metadata": {},
   "source": [
    "### Chunking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47dbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a48d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c1843cc021456688ea56efbc7180e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf,.docx,.txt', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload_widget = FileUpload(accept='.pdf,.docx,.txt', multiple=False)\n",
    "display(upload_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c5dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sejal\n",
      "[nltk_data]     Hanmante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sejal\n",
      "[nltk_data]     Hanmante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sejal\n",
      "[nltk_data]     Hanmante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Sejal\n",
      "[nltk_data]     Hanmante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abc5c630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8affe672a57544f58b0ec94d896890a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileUpload(value=(), accept='.pdf,.docx,.txt', description='Upload'), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec4ba44a15e466ba928243ace3ed9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileUpload(value=(), accept='.pdf,.docx,.txt', description='Upload'), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from ipywidgets import FileUpload, Output, VBox\n",
    "\n",
    "\n",
    "# ✅ Define text extractors\n",
    "def extract_text_from_pdf(file_stream):\n",
    "    with pdfplumber.open(file_stream) as pdf:\n",
    "        return '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "\n",
    "def extract_text_from_docx(file_stream):\n",
    "    doc = docx.Document(file_stream)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_txt(file_stream):\n",
    "    return file_stream.read().decode('utf-8')\n",
    "\n",
    "# ✅ General extractor\n",
    "def extract_text(uploaded_file):\n",
    "    file_info = uploaded_file.value[0]\n",
    "    filename = file_info['name']\n",
    "    content = file_info['content']\n",
    "    ext = filename.split('.')[-1].lower()\n",
    "    file_stream = io.BytesIO(content)\n",
    "\n",
    "    if ext == 'pdf':\n",
    "        return extract_text_from_pdf(file_stream)\n",
    "    elif ext == 'docx':\n",
    "        return extract_text_from_docx(file_stream)\n",
    "    elif ext == 'txt':\n",
    "        return extract_text_from_txt(file_stream)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "# ✅ Text chunker\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = words[start:end]\n",
    "        chunks.append(' '.join(chunk))\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# ✅ Output box\n",
    "output_box = Output()\n",
    "\n",
    "# ✅ Function to run on file upload\n",
    "def on_file_upload(change):\n",
    "    output_box.clear_output()\n",
    "    with output_box:\n",
    "        try:\n",
    "            if upload_widget.value:\n",
    "                raw_text = extract_text(upload_widget)\n",
    "                chunks = chunk_text(raw_text)\n",
    "\n",
    "                print(f\"✅ Total Chunks: {len(chunks)}\")\n",
    "                for i, chunk in enumerate(chunks[:5]):\n",
    "                    print(f\"\\n--- Chunk {i+1} ---\\n{chunk[:500]}...\")\n",
    "            else:\n",
    "                print(\"⚠️ No file uploaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "\n",
    "# ✅ Upload widget\n",
    "upload_widget = FileUpload(accept='.pdf,.docx,.txt', multiple=False)\n",
    "upload_widget.observe(on_file_upload, names='value')\n",
    "\n",
    "# ✅ Show UI\n",
    "display(VBox([upload_widget, output_box]))\n",
    "\n",
    "# ✅ Define text extractors\n",
    "def extract_text_from_pdf(file_stream):\n",
    "    with pdfplumber.open(file_stream) as pdf:\n",
    "        return '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "\n",
    "def extract_text_from_docx(file_stream):\n",
    "    doc = docx.Document(file_stream)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_txt(file_stream):\n",
    "    return file_stream.read().decode('utf-8')\n",
    "\n",
    "# ✅ General extractor\n",
    "def extract_text(uploaded_file):\n",
    "    file_info = uploaded_file.value[0]\n",
    "    filename = file_info['name']\n",
    "    content = file_info['content']\n",
    "    ext = filename.split('.')[-1].lower()\n",
    "    file_stream = io.BytesIO(content)\n",
    "\n",
    "    if ext == 'pdf':\n",
    "        return extract_text_from_pdf(file_stream)\n",
    "    elif ext == 'docx':\n",
    "        return extract_text_from_docx(file_stream)\n",
    "    elif ext == 'txt':\n",
    "        return extract_text_from_txt(file_stream)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "# ✅ Text chunker\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = words[start:end]\n",
    "        chunks.append(' '.join(chunk))\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# ✅ Output box\n",
    "output_box = Output()\n",
    "\n",
    "# ✅ Function to run on file upload\n",
    "def on_file_upload(change):\n",
    "    output_box.clear_output()\n",
    "    with output_box:\n",
    "        try:\n",
    "            if upload_widget.value:\n",
    "                raw_text = extract_text(upload_widget)\n",
    "                chunks = chunk_text(raw_text)\n",
    "\n",
    "                print(f\"✅ Total Chunks: {len(chunks)}\")\n",
    "                for i, chunk in enumerate(chunks[:5]):\n",
    "                    print(f\"\\n--- Chunk {i+1} ---\\n{chunk[:500]}...\")\n",
    "            else:\n",
    "                print(\"⚠️ No file uploaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "\n",
    "# ✅ Upload widget\n",
    "upload_widget = FileUpload(accept='.pdf,.docx,.txt', multiple=False)\n",
    "upload_widget.observe(on_file_upload, names='value')\n",
    "\n",
    "# ✅ Show UI\n",
    "display(VBox([upload_widget, output_box]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e227c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Please upload a file.\n"
     ]
    }
   ],
   "source": [
    "# 🔁 Call this manually after uploading\n",
    "process_uploaded_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b142ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.2.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading numpy-2.2.4-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.6 MB 3.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 5.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.1/12.6 MB 5.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.0/12.6 MB 6.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.8/12.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.6/12.6 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.4/12.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 5.0 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Installing collected packages: safetensors, numpy, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 numpy-2.2.4 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"pip install transformers\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b82c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from torch) (75.8.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Using cached torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"pip install torch \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3e3d88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d06ce",
   "metadata": {},
   "source": [
    "### InsuranceBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877deeb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.timm_wrapper.configuration_timm_wrapper because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1968\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\timm_wrapper\\configuration_timm_wrapper.py:25\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_timm_available():\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageNetInfo, infer_imagenet_subset\n\u001b[0;32m     28\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\timm\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scriptable, is_exportable, set_scriptable, set_exportable\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_model, list_models, list_pretrained, is_model, list_modules, model_entrypoint, \\\n\u001b[0;32m      4\u001b[0m     is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\timm\\layers\\__init__.py:8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblur_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlurPool2d, create_aa\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_classifier, ClassifierHead, NormMlpClassifierHead, ClNormMlpClassifierHead\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcond_conv2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CondConv2d, get_condconv_initializer\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\timm\\layers\\classifier.py:15\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate_act\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_act_layer\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate_norm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_norm_layer\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_pool\u001b[39m(\n\u001b[0;32m     19\u001b[0m         num_features: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m     20\u001b[0m         num_classes: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m         input_fmt: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m ):\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\timm\\layers\\create_norm.py:14\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnorm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d, RmsNorm, RmsNorm2d, SimpleNorm, SimpleNorm2d\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FrozenBatchNorm2d\n\u001b[0;32m     16\u001b[0m _NORM_MAP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     17\u001b[0m     batchnorm\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mBatchNorm2d,\n\u001b[0;32m     18\u001b[0m     batchnorm2d\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mBatchNorm2d,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     frozenbatchnorm2d\u001b[38;5;241m=\u001b[39mFrozenBatchNorm2d,\n\u001b[0;32m     29\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torchvision\\_meta_registrations.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroi_align\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_roi_align\u001b[39m(\u001b[38;5;28minput\u001b[39m, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n\u001b[0;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(rois\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrois must have shape as Tensor[K, 5]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torchvision\\_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mextension\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[0;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllmware/industry-bert-insurance-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllmware/industry-bert-insurance-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:552\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_config_for_auto_class(config)\n\u001b[0;32m    551\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map\n\u001b[1;32m--> 552\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    553\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m resolve_trust_remote_code(\n\u001b[0;32m    554\u001b[0m     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\u001b[0;32m    555\u001b[0m )\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Set the adapter kwargs\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:790\u001b[0m, in \u001b[0;36m_LazyAutoMapping.keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     mapping_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, name)\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    792\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    793\u001b[0m     ]\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:786\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 786\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name], attr)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:702\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m--> 702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, attr):\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1956\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1954\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1956\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1957\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1970\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1971\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1972\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1973\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.timm_wrapper.configuration_timm_wrapper because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# Load InsuranceBERT model\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "model = AutoModel.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    embeddings = output.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall timm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f9646",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', ' ', text)  # remove newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)  # normalize whitespace\n",
    "    text = re.sub(r'Page\\s*\\d+|\\d+\\s*/\\s*\\d+', '', text)  # remove common page numbers\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42f5e3",
   "metadata": {},
   "source": [
    "### Processing all pdfs in the folder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = r\"D:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\policy_pdfs\"\n",
    "all_chunks = []\n",
    "all_embeddings = []\n",
    "metadata = []\n",
    "\n",
    "for filename in tqdm(os.listdir(pdf_folder)):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        path = os.path.join(pdf_folder, filename)\n",
    "        raw_text = extract_text_from_pdf(path)\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        chunks = chunk_text(cleaned_text)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding = get_embedding(chunk)\n",
    "            all_chunks.append(chunk)\n",
    "            all_embeddings.append(embedding)\n",
    "            metadata.append({\"file\": filename, \"chunk_id\": i})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64bb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Save using pickle\n",
    "with open(\"embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"chunks\": all_chunks,\n",
    "        \"embeddings\": all_embeddings,\n",
    "        \"metadata\": metadata\n",
    "    }, f)\n",
    "\n",
    "print(\"✅ Embeddings and chunks saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bee651",
   "metadata": {},
   "source": [
    "### FAISS INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1952ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings, chunks, metadata\n",
    "with open(\"embeddings.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "all_embeddings = np.array(data[\"embeddings\"]).astype(\"float32\")\n",
    "all_chunks = data[\"chunks\"]\n",
    "metadata = data[\"metadata\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0081a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "dimension = all_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(all_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"policy_index_docs.faiss\")\n",
    "\n",
    "# Save metadata and chunks for lookup\n",
    "with open(\"policy_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"chunks\": all_chunks, \"metadata\": metadata}, f)\n",
    "\n",
    "print(\"✅ FAISS index and metadata saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index\n",
    "index = faiss.read_index(\"policy_index_docs.faiss\")\n",
    "\n",
    "# Load metadata & chunks\n",
    "with open(\"policy_metadata.pkl\", \"rb\") as f:\n",
    "    db = pickle.load(f)\n",
    "\n",
    "chunks = db[\"chunks\"]\n",
    "metadata = db[\"metadata\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dec645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_policy(query, k=3):\n",
    "    query_embedding = get_embedding(query).astype(\"float32\").reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    results = []\n",
    "    for i in indices[0]:\n",
    "        results.append({\n",
    "            \"chunk\": chunks[i],\n",
    "            \"file\": metadata[i][\"file\"],\n",
    "            \"chunk_id\": metadata[i][\"chunk_id\"]\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54b10f",
   "metadata": {},
   "source": [
    "### Query Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65282fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"policy_index.faiss\")\n",
    "\n",
    "# Load metadata\n",
    "with open(\"policy_metadata.pkl\", \"rb\") as f:\n",
    "    db = pickle.load(f)\n",
    "\n",
    "chunks = db[\"chunks\"]\n",
    "metadata = db[\"metadata\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff1e6e",
   "metadata": {},
   "source": [
    "### Query Search + prompt builder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a30e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, k=3):\n",
    "    query_embedding = get_embedding(query).astype(\"float32\").reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    results = []\n",
    "    for i in indices[0]:\n",
    "        results.append({\n",
    "            \"chunk\": chunks[i],\n",
    "            \"file\": metadata[i][\"file\"],\n",
    "            \"chunk_id\": metadata[i][\"chunk_id\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def build_prompt(retrieved_chunks, query):\n",
    "    context = \"\\n\\n\".join([f\"[{r['file']} - Chunk {r['chunk_id']}]:\\n{r['chunk']}\" for r in retrieved_chunks])\n",
    "    prompt = f\"\"\"You are a helpful assistant. Answer the user's question based only on the provided policy content.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a63496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def ask_deepseek(prompt, model=\"deepseek/deepseek-r1:free\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant for insurance policy queries.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    return response.choices[0].message['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0964b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Does this policy provide cashless hospitalization for critical illness?\"\n",
    "\n",
    "# Step 1: Retrieve\n",
    "top_chunks = retrieve_relevant_chunks(query)\n",
    "\n",
    "# Step 2: Build Prompt\n",
    "prompt = build_prompt(top_chunks, query)\n",
    "\n",
    "# Step 3: Ask LLM\n",
    "answer = ask_deepseek(prompt)\n",
    "\n",
    "# Step 4: Display\n",
    "print(\"📄 Retrieved Context:\")\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"\\n[{chunk['file']} - Chunk {chunk['chunk_id']}]:\\n{chunk['chunk'][:400]}...\\n\")\n",
    "\n",
    "print(\"\\n🧠 Answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72027a6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20ba022a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39d61f27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "165d917a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef3cd628",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5677bf9a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
