{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dab9c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in d:\\anaconda3\\lib\\site-packages (0.11.4)\n",
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: nltk in d:\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: langchain in d:\\anaconda3\\lib\\site-packages (0.1.13)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in d:\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in d:\\anaconda3\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in d:\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in d:\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in d:\\anaconda3\\lib\\site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in d:\\anaconda3\\lib\\site-packages (from python-docx) (4.13.1)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\anaconda3\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\anaconda3\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\anaconda3\\lib\\site-packages (from langchain) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in d:\\anaconda3\\lib\\site-packages (from langchain) (0.0.29)\n",
      "Collecting langchain-core<0.2.0,>=0.1.33 (from langchain)\n",
      "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in d:\\anaconda3\\lib\\site-packages (from langchain) (0.0.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\anaconda3\\lib\\site-packages (from langchain) (0.1.139)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\anaconda3\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\anaconda3\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (23.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.23.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: anyio in d:\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Using cached langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
      "Installing collected packages: python-docx, langchain-core\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.51\n",
      "    Uninstalling langchain-core-0.3.51:\n",
      "      Successfully uninstalled langchain-core-0.3.51\n",
      "Successfully installed langchain-core-0.1.53 python-docx-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-experimental 0.3.3 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.0.29 which is incompatible.\n",
      "langchain-experimental 0.3.3 requires langchain-core<0.4.0,>=0.3.15, but you have langchain-core 0.1.53 which is incompatible.\n",
      "langchain-google-genai 2.0.6 requires langchain-core<0.4,>=0.3.15, but you have langchain-core 0.1.53 which is incompatible.\n",
      "langgraph-checkpoint 2.0.24 requires langchain-core<0.4,>=0.2.38, but you have langchain-core 0.1.53 which is incompatible.\n",
      "langgraph-prebuilt 0.1.8 requires langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43, but you have langchain-core 0.1.53 which is incompatible.\n",
      "lib-resume-builder-aihawk 0.2 requires regex==2024.7.24, but you have regex 2023.12.25 which is incompatible.\n",
      "lib-resume-builder-aihawk 0.2 requires selenium==4.9.1, but you have selenium 4.27.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber python-docx nltk langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5525cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sejal\n",
      "[nltk_data]     Hanmante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5638e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sejal\n",
      "[nltk_data]     Hanmante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from ipywidgets import FileUpload\n",
    "import io\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0716cb",
   "metadata": {},
   "source": [
    "###  Text Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717b9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_stream):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_stream) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file_stream):\n",
    "    doc = docx.Document(file_stream)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_txt(file_stream):\n",
    "    return file_stream.read().decode('utf-8')\n",
    "\n",
    "def extract_text(uploaded_file):\n",
    "    filename = list(uploaded_file.value.keys())[0]\n",
    "    content = uploaded_file.value[filename]['content']\n",
    "    ext = filename.split('.')[-1].lower()\n",
    "    file_stream = io.BytesIO(content)\n",
    "\n",
    "    if ext == 'pdf':\n",
    "        return extract_text_from_pdf(file_stream)\n",
    "    elif ext == 'docx':\n",
    "        return extract_text_from_docx(file_stream)\n",
    "    elif ext == 'txt':\n",
    "        return extract_text_from_txt(file_stream)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please upload a PDF, DOCX, or TXT file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfae30",
   "metadata": {},
   "source": [
    "### Chunking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47dbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a48d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f798811e9994c8e8f7ccf9c502c0647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.pdf,.docx,.txt', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload_widget = FileUpload(accept='.pdf,.docx,.txt', multiple=False)\n",
    "display(upload_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc5c630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 152\n",
      "\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Policy wordings - Smart Super Health Insurance Policy PREAMBLE : The insurance cover provided under this Policy to the Insured / Insured Person up to the Sum Insured is and shall be subject to ( a ) the terms and conditions of this Policy and ( b ) the receipt of premium and ( c ) Disclosure to Information Norm ( including by way of the Proposal or Information Summary Sheet ) and ( d ) Schedule of Benefits . SECTION 1 - DEFINITIONS : Any word or expression to which a specific meaning has been as...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "and violent means . 1.2 ) `` Any one Illness '' means continuous period of Illness and it includes a relapse within 45 days from the date of last consultation with the Hospital/Nursing Home where treatment may have been taken . 1.3 ) `` Ayush Treatment '' refers to the medical and / or hospitalization treatments given under ‘ Ayurveda , Yoga and Naturopathy , Unani , Siddha and Homeopathy systems . 1.4 ) `` Cashless facility '' means a facility extended by the Insurer to the Insured where , the ...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "to call upon other Insurers , liable to the same Insured , to share the cost of an indemnity claim on a ratable proportion of Sum Insured . This clause shall not apply to any benefit offered on fixed benefit basis . 1.7 ) `` Condition Precedent '' shall mean a Policy term or condition upon which the Insurer's liability under the Policy is conditional upon . 1.8 ) `` Congenital Anomaly '' refers to a condition ( s ) which is present since birth , and which is abnormal with reference to form , str...\n",
      "\n",
      "--- Chunk 4 ---\n",
      "1.9 ) `` Cumulative Bonus '' shall mean any increase in the Sum Insured granted by the Insurer without an associated increase in the premium . 1.10 ) `` Day Care treatment '' means medical treatment , and / or surgical procedure which is : 1 Policy wordings – Smart Super Health Insurance Policy UIN : Bharti AXA General Insurance Company Limited , 1st Floor , Ferns Icon , Survey No.28 , Doddanekundi , Bangalore – 560037 . Telephone : + 91 80 49123900 1.10.1 ) undertaken under general or local ane...\n",
      "\n",
      "--- Chunk 5 ---\n",
      "established for day care treatment of Illness and / or injuries or a medical setup within a hospital and which has been registered with the local authorities , wherever applicable , and is under the supervision of a registered and qualified medical practitioner and must comply with all minimum criteria as under : i. has qualified nursing staff under its employment ii . has qualified medical practitioner/s in charge ; iii . has a fully equipped operation theatre of its own where surgical procedur...\n"
     ]
    }
   ],
   "source": [
    "if upload_widget.value:\n",
    "    raw_text = extract_text(upload_widget)\n",
    "    chunks = chunk_text(raw_text)\n",
    "\n",
    "    print(f\"Total Chunks: {len(chunks)}\\n\")\n",
    "    for i, chunk in enumerate(chunks[:5]):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\\n{chunk[:500]}...\")  # Truncate long chunks\n",
    "else:\n",
    "    print(\"Please upload a file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e3d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d06ce",
   "metadata": {},
   "source": [
    "### InsuranceBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "877deeb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.timm_wrapper.configuration_timm_wrapper because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1968\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\timm_wrapper\\configuration_timm_wrapper.py:25\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_timm_available():\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageNetInfo, infer_imagenet_subset\n\u001b[0;32m     28\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\timm\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scriptable, is_exportable, set_scriptable, set_exportable\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_model, list_models, list_pretrained, is_model, list_modules, model_entrypoint, \\\n\u001b[0;32m      4\u001b[0m     is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\timm\\layers\\__init__.py:8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblur_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlurPool2d, create_aa\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_classifier, ClassifierHead, NormMlpClassifierHead, ClNormMlpClassifierHead\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcond_conv2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CondConv2d, get_condconv_initializer\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\timm\\layers\\classifier.py:15\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate_act\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_act_layer\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate_norm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_norm_layer\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_pool\u001b[39m(\n\u001b[0;32m     19\u001b[0m         num_features: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m     20\u001b[0m         num_classes: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m         input_fmt: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m ):\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\timm\\layers\\create_norm.py:14\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnorm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d, RmsNorm, RmsNorm2d, SimpleNorm, SimpleNorm2d\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FrozenBatchNorm2d\n\u001b[0;32m     16\u001b[0m _NORM_MAP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     17\u001b[0m     batchnorm\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mBatchNorm2d,\n\u001b[0;32m     18\u001b[0m     batchnorm2d\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mBatchNorm2d,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     frozenbatchnorm2d\u001b[38;5;241m=\u001b[39mFrozenBatchNorm2d,\n\u001b[0;32m     29\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torchvision\\_meta_registrations.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroi_align\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_roi_align\u001b[39m(\u001b[38;5;28minput\u001b[39m, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n\u001b[0;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(rois\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrois must have shape as Tensor[K, 5]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torchvision\\_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mextension\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[0;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllmware/industry-bert-insurance-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllmware/industry-bert-insurance-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:552\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_config_for_auto_class(config)\n\u001b[0;32m    551\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map\n\u001b[1;32m--> 552\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    553\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m resolve_trust_remote_code(\n\u001b[0;32m    554\u001b[0m     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\u001b[0;32m    555\u001b[0m )\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Set the adapter kwargs\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:790\u001b[0m, in \u001b[0;36m_LazyAutoMapping.keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     mapping_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, name)\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    792\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    793\u001b[0m     ]\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:786\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 786\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name], attr)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:702\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m--> 702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, attr):\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1956\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1954\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1956\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1957\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1970\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1971\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1972\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1973\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.timm_wrapper.configuration_timm_wrapper because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# Load InsuranceBERT model\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "model = AutoModel.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    embeddings = output.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall timm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f9646",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "885b854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', ' ', text)  # remove newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)  # normalize whitespace\n",
    "    text = re.sub(r'Page\\s*\\d+|\\d+\\s*/\\s*\\d+', '', text)  # remove common page numbers\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42f5e3",
   "metadata": {},
   "source": [
    "### Processing all pdfs in the folder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = r\"D:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\policy_pdfs\"\n",
    "all_chunks = []\n",
    "all_embeddings = []\n",
    "metadata = []\n",
    "\n",
    "for filename in tqdm(os.listdir(pdf_folder)):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        path = os.path.join(pdf_folder, filename)\n",
    "        raw_text = extract_text_from_pdf(path)\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        chunks = chunk_text(cleaned_text)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding = get_embedding(chunk)\n",
    "            all_chunks.append(chunk)\n",
    "            all_embeddings.append(embedding)\n",
    "            metadata.append({\"file\": filename, \"chunk_id\": i})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64bb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Save using pickle\n",
    "with open(\"embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"chunks\": all_chunks,\n",
    "        \"embeddings\": all_embeddings,\n",
    "        \"metadata\": metadata\n",
    "    }, f)\n",
    "\n",
    "print(\"✅ Embeddings and chunks saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bee651",
   "metadata": {},
   "source": [
    "### FAISS INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1952ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings, chunks, metadata\n",
    "with open(\"embeddings.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "all_embeddings = np.array(data[\"embeddings\"]).astype(\"float32\")\n",
    "all_chunks = data[\"chunks\"]\n",
    "metadata = data[\"metadata\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0081a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "dimension = all_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(all_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"policy_index.faiss\")\n",
    "\n",
    "# Save metadata and chunks for lookup\n",
    "with open(\"policy_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"chunks\": all_chunks, \"metadata\": metadata}, f)\n",
    "\n",
    "print(\"✅ FAISS index and metadata saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index\n",
    "index = faiss.read_index(\"policy_index.faiss\")\n",
    "\n",
    "# Load metadata & chunks\n",
    "with open(\"policy_metadata.pkl\", \"rb\") as f:\n",
    "    db = pickle.load(f)\n",
    "\n",
    "chunks = db[\"chunks\"]\n",
    "metadata = db[\"metadata\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dec645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_policy(query, k=3):\n",
    "    query_embedding = get_embedding(query).astype(\"float32\").reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    results = []\n",
    "    for i in indices[0]:\n",
    "        results.append({\n",
    "            \"chunk\": chunks[i],\n",
    "            \"file\": metadata[i][\"file\"],\n",
    "            \"chunk_id\": metadata[i][\"chunk_id\"]\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54b10f",
   "metadata": {},
   "source": [
    "### Query Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65282fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"policy_index.faiss\")\n",
    "\n",
    "# Load metadata\n",
    "with open(\"policy_metadata.pkl\", \"rb\") as f:\n",
    "    db = pickle.load(f)\n",
    "\n",
    "chunks = db[\"chunks\"]\n",
    "metadata = db[\"metadata\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff1e6e",
   "metadata": {},
   "source": [
    "### Query Search + prompt builder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a30e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, k=3):\n",
    "    query_embedding = get_embedding(query).astype(\"float32\").reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    results = []\n",
    "    for i in indices[0]:\n",
    "        results.append({\n",
    "            \"chunk\": chunks[i],\n",
    "            \"file\": metadata[i][\"file\"],\n",
    "            \"chunk_id\": metadata[i][\"chunk_id\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def build_prompt(retrieved_chunks, query):\n",
    "    context = \"\\n\\n\".join([f\"[{r['file']} - Chunk {r['chunk_id']}]:\\n{r['chunk']}\" for r in retrieved_chunks])\n",
    "    prompt = f\"\"\"You are a helpful assistant. Answer the user's question based only on the provided policy content.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a63496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def ask_deepseek(prompt, model=\"deepseek/deepseek-r1:free\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant for insurance policy queries.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    return response.choices[0].message['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0964b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Does this policy provide cashless hospitalization for critical illness?\"\n",
    "\n",
    "# Step 1: Retrieve\n",
    "top_chunks = retrieve_relevant_chunks(query)\n",
    "\n",
    "# Step 2: Build Prompt\n",
    "prompt = build_prompt(top_chunks, query)\n",
    "\n",
    "# Step 3: Ask LLM\n",
    "answer = ask_deepseek(prompt)\n",
    "\n",
    "# Step 4: Display\n",
    "print(\"📄 Retrieved Context:\")\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"\\n[{chunk['file']} - Chunk {chunk['chunk_id']}]:\\n{chunk['chunk'][:400]}...\\n\")\n",
    "\n",
    "print(\"\\n🧠 Answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72027a6d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
