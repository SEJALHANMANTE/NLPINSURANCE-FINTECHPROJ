{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9d22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78ca1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0a33b487ae4b76a84b730e8f060255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf,.docx,.txt', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "import spacy\n",
    "from ipywidgets import FileUpload\n",
    "import io\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# File uploader widget\n",
    "upload_widget = FileUpload(accept='.pdf,.docx,.txt', multiple=False)\n",
    "\n",
    "# Display widget\n",
    "display(upload_widget)\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(file_stream):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_stream) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or ''\n",
    "    return text\n",
    "\n",
    "# Extract text from DOCX\n",
    "def extract_text_from_docx(file_stream):\n",
    "    doc = docx.Document(file_stream)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Extract text from TXT\n",
    "def extract_text_from_txt(file_stream):\n",
    "    return file_stream.read().decode('utf-8')\n",
    "\n",
    "# Main text extractor\n",
    "def extract_text(uploaded_file):\n",
    "    file_info = uploaded_file.value[0]\n",
    "    filename = file_info['name']\n",
    "    content = file_info['content']\n",
    "    ext = filename.split('.')[-1].lower()\n",
    "    file_stream = io.BytesIO(content)\n",
    "\n",
    "    if ext == 'pdf':\n",
    "        return extract_text_from_pdf(file_stream)\n",
    "    elif ext == 'docx':\n",
    "        return extract_text_from_docx(file_stream)\n",
    "    elif ext == 'txt':\n",
    "        return extract_text_from_txt(file_stream)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please upload a PDF, DOCX, or TXT file.\")\n",
    "\n",
    "# Chunk text using SpaCy\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = [token.text for token in nlp(text) if not token.is_space]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = words[start:end]\n",
    "        chunks.append(' '.join(chunk))\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d8ebbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Please upload a file.\n"
     ]
    }
   ],
   "source": [
    "# Main logic\n",
    "def process_file():\n",
    "    if upload_widget.value:\n",
    "        try:\n",
    "            raw_text = extract_text(upload_widget)\n",
    "            chunks = chunk_text(raw_text)\n",
    "            print(f\"âœ… Total Chunks: {len(chunks)}\\n\")\n",
    "            for i, chunk in enumerate(chunks[:5]):\n",
    "                print(f\"\\n--- Chunk {i+1} ---\\n{chunk[:500]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Please upload a file.\")\n",
    "\n",
    "# Run the process\n",
    "process_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96617374",
   "metadata": {},
   "source": [
    "### Insurance BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f2f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6572a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InsuranceBERT model\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "model = AutoModel.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    embeddings = output.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac9eb6",
   "metadata": {},
   "source": [
    "### Processing all docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc11bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import faiss\n",
    "import spacy\n",
    "import pdfplumber\n",
    "import docx\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load spaCy and InsuranceBERT\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "model = AutoModel.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "\n",
    "# Utility to extract text\n",
    "def extract_text(file_path):\n",
    "    ext = file_path.split('.')[-1].lower()\n",
    "    if ext == 'pdf':\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            return ' '.join([page.extract_text() or '' for page in pdf.pages])\n",
    "    elif ext == 'docx':\n",
    "        doc = docx.Document(file_path)\n",
    "        return '\\n'.join([p.text for p in doc.paragraphs])\n",
    "    elif ext == 'txt':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "# Utility to chunk text\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if not token.is_space]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(' '.join(words[start:end]))\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Get embedding for each chunk\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Extract policy name from text (heuristic: first few lines)\n",
    "def extract_policy_name(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line in lines[:10]:\n",
    "        if \"policy\" in line.lower() or \"plan\" in line.lower():\n",
    "            return line.strip()\n",
    "    return \"Unknown_Policy\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c78c85",
   "metadata": {},
   "source": [
    "#### Process FOlder and build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84a6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def build_faiss_index(folder_path):\n",
    "    index = faiss.IndexFlatL2(768)\n",
    "    chunk_metadata = []  # stores (policy_name, file_name, chunk_text)\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith(('.pdf', '.docx', '.txt')):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                full_text = extract_text(file_path)\n",
    "                policy_name = extract_policy_name(full_text)\n",
    "                chunks = chunk_text(full_text)\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    embedding = get_embedding(chunk)\n",
    "                    index.add(np.array([embedding], dtype='float32'))\n",
    "                    chunk_metadata.append((policy_name, filename, chunk))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error processing {filename}: {e}\")\n",
    "\n",
    "    return index, chunk_metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edecbf",
   "metadata": {},
   "source": [
    "#### Save and load Index + Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd15adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save index and metadata\n",
    "def save_index(index, metadata, index_path='faiss_index.index', metadata_path='metadata.pkl'):\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "# Load index and metadata\n",
    "def load_index(index_path='faiss_index.index', metadata_path='metadata.pkl'):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(metadata_path, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return index, metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d662ce",
   "metadata": {},
   "source": [
    "#### Query and retrieve relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b22df",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def search_index(query, index, metadata, top_k=5):\n",
    "    query_emb = get_embedding(query).reshape(1, -1).astype('float32')\n",
    "    D, I = index.search(query_emb, top_k)\n",
    "\n",
    "    results = []\n",
    "    for i in I[0]:\n",
    "        if i < len(metadata):\n",
    "            results.append(metadata[i])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6595aafb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 95-96: malformed \\N character escape (1776261985.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"# Replace with your actual path to the folder containing the 20 policy files\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 95-96: malformed \\N character escape\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Replace with your actual path to the folder containing the 20 policy files\n",
    "folder_path = r\"D:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\policy_pdfs\"\n",
    "\n",
    "# Build FAISS Index and metadata\n",
    "index, metadata = build_faiss_index(folder_path)\n",
    "\n",
    "# Save for future use\n",
    "save_index(index, metadata)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199870f2",
   "metadata": {},
   "source": [
    "#### Load and Query the FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved index and metadata\n",
    "index, metadata = load_index()\n",
    "\n",
    "# Sample user query\n",
    "user_query = \"What is the waiting period for pre-existing conditions?\"\n",
    "\n",
    "# Search top 5 relevant chunks\n",
    "results = search_index(user_query, index, metadata)\n",
    "\n",
    "# Display results\n",
    "for i, (policy_name, filename, chunk) in enumerate(results, 1):\n",
    "    print(f\"\\nðŸ”¹ Result {i}\")\n",
    "    print(f\"ðŸ“„ Policy: {policy_name}\")\n",
    "    print(f\"ðŸ“ File: {filename}\")\n",
    "    print(f\"ðŸ“œ Text: {chunk[:500]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042c8b4",
   "metadata": {},
   "source": [
    "#### Check Policy name in the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the user uploads a new PDF\n",
    "user_uploaded_file = r\"C:\\Users\\Sejal Hanmante\\Downloads\\healthplus-policydocument.pdf\"\n",
    "\n",
    "# Extract text & policy name\n",
    "text = extract_text(user_uploaded_file)\n",
    "policy_name = extract_policy_name(text)\n",
    "\n",
    "# Check if this policy name exists in your metadata\n",
    "existing_policy_names = set([meta[0] for meta in metadata])\n",
    "\n",
    "if policy_name in existing_policy_names:\n",
    "    print(f\"âœ… Policy '{policy_name}' exists in the database.\")\n",
    "    # You can now directly query the FAISS index using `search_index`\n",
    "else:\n",
    "    print(\"âŒ Policy not found, reprocessing...\")\n",
    "    # Call the same logic as in `build_faiss_index` to chunk and embed on-the-fly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527dbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = {\n",
    "    \"What is the sum insured under this plan?\": \"Sum Insured\",\n",
    "    \"How long is the waiting period for diabetes?\": \"Waiting Period\",\n",
    "    \"What OPD cover is provided?\": \"OPD Coverage\"\n",
    "}\n",
    "\n",
    "for query, expected in test_queries.items():\n",
    "    results = search_index(query, index, metadata)\n",
    "    print(f\"\\nðŸ§ª Query: {query}\")\n",
    "    match_found = any(expected.lower() in chunk.lower() for _, _, chunk in results)\n",
    "    print(f\"âœ… Match found: {match_found}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29989a0e",
   "metadata": {},
   "source": [
    "#### Eval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153da84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53304ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_retrieval(test_queries, index, metadata, top_k=5):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    hit_at_k = []\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for query, expected_keyword in test_queries.items():\n",
    "        results = search_index(query, index, metadata, top_k=top_k)\n",
    "\n",
    "        retrieved_texts = [chunk.lower() for _, _, chunk in results]\n",
    "        match_flags = [int(expected_keyword.lower() in text) for text in retrieved_texts]\n",
    "\n",
    "        # Metrics\n",
    "        precision = sum(match_flags) / top_k\n",
    "        recall = 1 if any(match_flags) else 0\n",
    "        rank = next((i + 1 for i, match in enumerate(match_flags) if match), None)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        hit_at_k.append(recall)\n",
    "        reciprocal_ranks.append(1 / rank if rank else 0)\n",
    "\n",
    "        print(f\"\\nðŸ” Query: {query}\")\n",
    "        print(f\"âœ… Keyword matched in top-{top_k}: {bool(rank)} at rank {rank if rank else '-'}\")\n",
    "\n",
    "    # Aggregate Metrics\n",
    "    print(\"\\nðŸ“Š Evaluation Results:\")\n",
    "    print(f\"ðŸ”¹ Precision@{top_k}: {np.mean(precision_list):.2f}\")\n",
    "    print(f\"ðŸ”¹ Recall@{top_k}: {np.mean(recall_list):.2f}\")\n",
    "    print(f\"ðŸ”¹ F1 Score: {f1_score(recall_list, [1]*len(recall_list)):.2f}\")\n",
    "    print(f\"ðŸ”¹ MRR: {np.mean(reciprocal_ranks):.2f}\")\n",
    "    print(f\"ðŸ”¹ Hit@{top_k}: {np.mean(hit_at_k):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "index, metadata = load_index()  # Load saved FAISS + meta\n",
    "evaluate_retrieval(test_queries, index, metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
