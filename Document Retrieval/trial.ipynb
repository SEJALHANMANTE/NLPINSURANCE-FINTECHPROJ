{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9d22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c421642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: nltk in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: sentence-transformers in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from bert-score) (2.6.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from bert-score) (4.51.2)\n",
      "Requirement already satisfied: numpy in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from bert-score) (2.2.4)\n",
      "Requirement already satisfied: requests in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from bert-score) (4.67.1)\n",
      "Collecting matplotlib (from bert-score)\n",
      "  Using cached matplotlib-3.10.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from bert-score) (24.2)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: scikit-learn in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: networkx in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from torch>=1.0.0->bert-score) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert-score)\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert-score)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert-score)\n",
      "  Using cached fonttools-4.57.0-cp312-cp312-win_amd64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->bert-score)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from matplotlib->bert-score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests->bert-score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests->bert-score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from requests->bert-score) (2025.1.31)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\nlpinsuranceproject\\nlpinsurance-fintechproj\\.conda\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Using cached absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Using cached matplotlib-3.10.1-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.57.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, absl-py, matplotlib, rouge-score, bert-score\n",
      "Successfully installed absl-py-2.2.2 bert-score-0.3.13 contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 rouge-score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bert-score rouge-score nltk sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb169cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Extracting and chunking text...\n",
      "📊 Embedding and indexing chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 164/164 [00:38<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed and indexed 164 chunks.\n",
      "🤖 Thinking.....\n",
      "🔍 Searching for relevant clauses...\n",
      "\n",
      "Result (Cosine Similarity: 0.5088):\n",
      "Migration means a facility provided to Policyholders (including all \n",
      "members under family cover and group policies), to transfer the \n",
      "credits gained for pre-existing diseases and specific waiting \n",
      "periods from one health insurance policy to another with the \n",
      "same insurer. Newborn baby means baby born during the Policy Period and is \n",
      "aged between 1 day and 90 days, both days inclusive. Network Provider means hospitals or health care providers \n",
      "enlisted by an Insurer or by a TPA and Insurer together to provide \n",
      "medical services to an Insured on payment by a cashless facility.\n",
      "\n",
      "📝 Summarizing result with OpenRouter...\n",
      "\n",
      "🔑 Summary of the top result:\n",
      " **Summary of Insurance Policy Clauses:**\n",
      "\n",
      "The clause describes three insurance-related terms:\n",
      "\n",
      "1. **Migration**: A facility allowing policyholders to transfer credits gained for pre-existing diseases and waiting periods to a new health insurance policy with the same insurer.\n",
      "2. **Newborn Baby**: A baby born during the policy period, aged between 1 and 90 days.\n",
      "3. **Network Provider**: Hospitals or healthcare providers enlisted by the insurer or TPA (Third-Party Administrator) to provide medical services to insured individuals on a cashless basis.\n",
      "\n",
      "**Explanation in Easy-to-Understand Words:**\n",
      "\n",
      "1. **Migration**: Imagine you have a health insurance policy with a particular company, and you want to switch to a different plan with the same company. If you've already served the waiting period for certain health conditions (like diabetes or hypertension), you wouldn't want to start from scratch with a new waiting period, right? The \"migration\" facility allows you to carry forward the credits you've earned towards those waiting periods to your new policy. This way, you can enjoy continuous coverage without having to restart the waiting period.\n",
      "\n",
      "2. **Newborn Baby**: This refers to a baby born during the time your insurance policy is active (the policy period). The baby is eligible for coverage under your policy as long as they are between 1 day and 90 days old.\n",
      "\n",
      "3. **Network Provider**: When you need medical treatment, you don't want to pay out of pocket, right? A \"network provider\" is a hospital or healthcare service provider that has a tie-up with your insurance company. This means that if you go to one of these network providers for treatment, your insurance company will pay the bill directly, so you don't have to pay upfront. It's like having a pre-arranged agreement for cashless medical care.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import faiss\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI  # OpenRouter Client\n",
    "\n",
    "# ---------- Global Models & Setup ----------\n",
    "\n",
    "# Load SpaCy model for sentence segmentation\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load BERT model and tokenizer for insurance-specific embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "bert_model = AutoModel.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "\n",
    "# Initialize FAISS index with the embedding dimension\n",
    "embedding_dim = 768\n",
    "faiss_index = faiss.IndexFlatIP(embedding_dim)  # Cosine similarity-based index\n",
    "policy_chunk_map = []  # Holds chunks for the current policy\n",
    "\n",
    "# Initialize OpenRouter client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"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\",  # Replace with your actual API key\n",
    ")\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF (fitz).\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "def chunk_policy_text(text, chunk_size=3):\n",
    "    \"\"\"Chunk the policy text into smaller segments.\"\"\"\n",
    "    doc = spacy_model(text)\n",
    "    sentences = [\n",
    "        sent.text.strip()\n",
    "        for sent in doc.sents\n",
    "        if len(sent.text.strip()) > 50 and not sent.text.lower().startswith((\"sbi general\", \"registered office\"))\n",
    "    ]\n",
    "    chunks = [\" \".join(sentences[i:i + chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def embed(text: str):\n",
    "    \"\"\"Generate BERT embeddings for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = bert_model(**inputs)\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "def normalize_vectors(vectors):\n",
    "    \"\"\"Normalize vectors for FAISS search.\"\"\"\n",
    "    vectors = np.array(vectors).astype(np.float32)\n",
    "    faiss.normalize_L2(vectors)\n",
    "    return vectors\n",
    "\n",
    "# ---------- Processing & Indexing ----------\n",
    "\n",
    "def process_current_policy(pdf_path):\n",
    "    \"\"\"Process and index the current policy PDF.\"\"\"\n",
    "    global faiss_index, policy_chunk_map\n",
    "\n",
    "    print(\"📄 Extracting and chunking text...\")\n",
    "    text = extract_text(pdf_path)\n",
    "    chunks = chunk_policy_text(text)\n",
    "\n",
    "    print(\"📊 Embedding and indexing chunks...\")\n",
    "    chunk_vectors = [embed(chunk) for chunk in tqdm(chunks, desc=\"Embedding\")]\n",
    "    chunk_vectors = normalize_vectors(chunk_vectors)\n",
    "\n",
    "    faiss_index.add(chunk_vectors)\n",
    "    policy_chunk_map = chunks  # Store chunks for retrieval\n",
    "\n",
    "    print(f\"✅ Processed and indexed {len(chunks)} chunks.\")\n",
    "\n",
    "# ---------- Querying ----------\n",
    "\n",
    "def search_policy(query, top_k=1):\n",
    "    \"\"\"Search the policy for the most relevant clause.\"\"\"\n",
    "    print(\"🤖 Thinking\", end=\"\")\n",
    "    for _ in range(5): \n",
    "        time.sleep(0.2); print(\".\", end=\"\", flush=True)\n",
    "    print(\"\\n🔍 Searching for relevant clauses...\")\n",
    "\n",
    "    query_vec = normalize_vectors([embed(query)])\n",
    "    D, I = faiss_index.search(query_vec, top_k)\n",
    "\n",
    "    # Check if the results are valid and not empty\n",
    "    if I.shape[0] > 0 and I[0].size > 0:\n",
    "        top_match = policy_chunk_map[I[0][0]]  # Access top match\n",
    "        score = D[0][0]  # The similarity score\n",
    "        print(f\"\\nResult (Cosine Similarity: {score:.4f}):\\n{top_match}\")\n",
    "    else:\n",
    "        print(\"❌ No relevant results found.\")\n",
    "        return None\n",
    "\n",
    "    summary = summarize_with_openrouter(top_match)\n",
    "    return summary\n",
    "\n",
    "def summarize_with_openrouter(text):\n",
    "    \"\"\"Summarize the top result using OpenRouter's LLM.\"\"\"\n",
    "    print(\"\\n📝 Summarizing result with OpenRouter...\")\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/llama-4-scout:free\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Summarize the following insurance policy clause: and also next explain it in easy to understand words\"},\n",
    "                    {\"type\": \"text\", \"text\": text}\n",
    "                ]\n",
    "            }]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during summarization: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- Execution ----------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"D:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\policy_pdfs\\c983715b738f4f88b4dd0fc684d796c3.pdf\"\n",
    "    \n",
    "    # Process the policy PDF\n",
    "    process_current_policy(pdf_path)\n",
    "\n",
    "    # Ask a query\n",
    "    result_summary = search_policy(\"Is newborn baby covered under this plan?\")\n",
    "    if result_summary:\n",
    "        print(\"\\n🔑 Summary of the top result:\\n\", result_summary)\n",
    "\n",
    "    # Clean-up (optional, but FAISS index will be destroyed after script ends)\n",
    "    #del faiss_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c849dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Processing 1. Policy- NMP.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Chunks: 100%|██████████| 148/148 [00:35<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: Is newborn baby covered under this plan?\n",
      "\n",
      "🔍 Query: What is the waiting period for pre-existing diseases?\n",
      "\n",
      "🔍 Query: Does this policy cover maternity expenses?\n",
      "\n",
      "🔍 Query: Are day care procedures included?\n",
      "\n",
      "🔍 Query: What is the cashless hospital network?\n",
      "\n",
      "🔍 Query: Is OPD treatment reimbursable?\n",
      "\n",
      "🔍 Query: What is the coverage amount for critical illness?\n",
      "\n",
      "🔍 Query: Are ambulance charges covered?\n",
      "\n",
      "🔍 Query: Is there a claim settlement ratio mentioned?\n",
      "\n",
      "🔍 Query: What documents are needed for claim filing?\n",
      "\n",
      "📄 Processing 242972d58c064559b7335ac1d9cdf9b5.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Chunks: 100%|██████████| 162/162 [00:42<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: Is newborn baby covered under this plan?\n",
      "\n",
      "🔍 Query: What is the waiting period for pre-existing diseases?\n",
      "\n",
      "🔍 Query: Does this policy cover maternity expenses?\n",
      "\n",
      "🔍 Query: Are day care procedures included?\n",
      "\n",
      "🔍 Query: What is the cashless hospital network?\n",
      "\n",
      "🔍 Query: Is OPD treatment reimbursable?\n",
      "\n",
      "🔍 Query: What is the coverage amount for critical illness?\n",
      "\n",
      "🔍 Query: Are ambulance charges covered?\n",
      "\n",
      "🔍 Query: Is there a claim settlement ratio mentioned?\n",
      "\n",
      "🔍 Query: What documents are needed for claim filing?\n",
      "\n",
      "📄 Processing 46045becd7b842dca4dfffd893ad9263.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Chunks: 100%|██████████| 82/82 [00:32<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: Is newborn baby covered under this plan?\n",
      "\n",
      "🔍 Query: What is the waiting period for pre-existing diseases?\n",
      "\n",
      "🔍 Query: Does this policy cover maternity expenses?\n",
      "\n",
      "🔍 Query: Are day care procedures included?\n",
      "\n",
      "🔍 Query: What is the cashless hospital network?\n",
      "\n",
      "🔍 Query: Is OPD treatment reimbursable?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: What is the coverage amount for critical illness?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: Are ambulance charges covered?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: Is there a claim settlement ratio mentioned?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: What documents are needed for claim filing?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "📄 Processing 572dc7b9faac4c3985d9782f791c3624.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Chunks: 100%|██████████| 312/312 [01:55<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: Is newborn baby covered under this plan?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: What is the waiting period for pre-existing diseases?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: Does this policy cover maternity expenses?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: Are day care procedures included?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: What is the cashless hospital network?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: Is OPD treatment reimbursable?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: What is the coverage amount for critical illness?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: Are ambulance charges covered?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: Is there a claim settlement ratio mentioned?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "🔍 Query: What documents are needed for claim filing?\n",
      "Summarization error: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 1/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 2/3 failed: 'NoneType' object is not subscriptable\n",
      "Evaluation retry 3/3 failed: 'NoneType' object is not subscriptable\n",
      "\n",
      "📄 Processing bc75b0756ad1487b9886aa0088809b25.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Chunks:  69%|██████▊   | 231/337 [01:05<00:29,  3.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 206\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename.endswith(\u001b[33m\"\u001b[39m\u001b[33m.pdf\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    205\u001b[39m         path = os.path.join(input_folder, filename)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         result = \u001b[43mprocess_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m         all_results.append(result)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mresults.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mprocess_policy\u001b[39m\u001b[34m(pdf_path, queries)\u001b[39m\n\u001b[32m    165\u001b[39m text = extract_text(pdf_path)\n\u001b[32m    166\u001b[39m chunks = chunk_policy_text(text)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m chunk_vectors = normalize_vectors([\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m tqdm(chunks, desc=\u001b[33m\"\u001b[39m\u001b[33mEmbedding Chunks\u001b[39m\u001b[33m\"\u001b[39m)])\n\u001b[32m    169\u001b[39m faiss_index = faiss.IndexFlatIP(embedding_dim)\n\u001b[32m    170\u001b[39m faiss_index.add(chunk_vectors)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36membed\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     56\u001b[39m inputs = tokenizer(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m512\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     output = \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output.last_hidden_state.mean(dim=\u001b[32m1\u001b[39m).squeeze().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1144\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1142\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1156\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1157\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import faiss\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# ---------- Setup ----------\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "bert_model = AutoModel.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "embedding_dim = 768\n",
    "\n",
    "# Sample queries\n",
    "queries = [\n",
    "    \"Is newborn baby covered under this plan?\",\n",
    "    \"What is the waiting period for pre-existing diseases?\",\n",
    "    \"Does this policy cover maternity expenses?\",\n",
    "    \"Are day care procedures included?\",\n",
    "    \"What is the cashless hospital network?\",\n",
    "    \"Is OPD treatment reimbursable?\",\n",
    "    \"What is the coverage amount for critical illness?\",\n",
    "    \"Are ambulance charges covered?\",\n",
    "    \"Is there a claim settlement ratio mentioned?\",\n",
    "    \"What documents are needed for claim filing?\"\n",
    "]\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "def chunk_policy_text(text, chunk_size=3):\n",
    "    doc = spacy_model(text)\n",
    "    sentences = [\n",
    "        sent.text.strip()\n",
    "        for sent in doc.sents\n",
    "        if len(sent.text.strip()) > 50 and not sent.text.lower().startswith((\"sbi general\", \"registered office\"))\n",
    "    ]\n",
    "    chunks = [\" \".join(sentences[i:i + chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def embed(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = bert_model(**inputs)\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "def normalize_vectors(vectors):\n",
    "    vectors = np.array(vectors).astype(np.float32)\n",
    "    faiss.normalize_L2(vectors)\n",
    "    return vectors\n",
    "\n",
    "def summarize_with_openrouter(text):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/llama-4-scout:free\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Summarize the following insurance policy clause and explain it simply:\"},\n",
    "                    {\"type\": \"text\", \"text\": text}\n",
    "                ]\n",
    "            }]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Summarization error: {e}\")\n",
    "        return \"Summarization failed.\"\n",
    "\n",
    "import re\n",
    "\n",
    "def safe_extract_scores(text):\n",
    "    \"\"\"\n",
    "    Extracts relevance, accuracy, clarity, helpfulness, and comments using regex from messy LLM output.\n",
    "    \"\"\"\n",
    "    def extract_number(label):\n",
    "        match = re.search(rf\"{label}\\s*[:\\-]?\\s*(\\d)\", text, re.IGNORECASE)\n",
    "        return int(match.group(1)) if match else 0\n",
    "\n",
    "    def extract_comment():\n",
    "        match = re.search(r\"comments?\\s*[:\\-]?\\s*(.*)\", text, re.IGNORECASE | re.DOTALL)\n",
    "        return match.group(1).strip() if match else \"No comment.\"\n",
    "\n",
    "    return {\n",
    "        \"relevance\": extract_number(\"relevance\"),\n",
    "        \"accuracy\": extract_number(\"accuracy\"),\n",
    "        \"clarity\": extract_number(\"clarity\"),\n",
    "        \"helpfulness\": extract_number(\"helpfulness\"),\n",
    "        \"comments\": extract_comment()\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_with_openrouter(query, chunk, summary, retries=3):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an evaluator for an insurance assistant system. Assess the following:\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Retrieved Chunk:\n",
    "{chunk}\n",
    "\n",
    "LLM-generated Summary:\n",
    "{summary}\n",
    "\n",
    "Rate the following from 0 to 5:\n",
    "- Relevance (does the chunk answer the query?)\n",
    "- Accuracy (is the summary faithful?)\n",
    "- Clarity (is it understandable?)\n",
    "- Helpfulness (would a user find it useful?)\n",
    "\n",
    "Also write a short evaluator comment.\n",
    "\n",
    "Respond like:\n",
    "Relevance: 4\n",
    "Accuracy: 3\n",
    "Clarity: 5\n",
    "Helpfulness: 4\n",
    "Comment: The summary is mostly relevant but lacks detail on OPD.\n",
    "    \"\"\"\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"meta-llama/llama-4-scout:free\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": eval_prompt}]\n",
    "                }]\n",
    "            )\n",
    "            response_text = completion.choices[0].message.content.strip()\n",
    "            return safe_extract_scores(response_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation retry {attempt+1}/{retries} failed: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    return {\n",
    "        \"relevance\": 0,\n",
    "        \"accuracy\": 0,\n",
    "        \"clarity\": 0,\n",
    "        \"helpfulness\": 0,\n",
    "        \"comments\": \"Evaluation failed completely.\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Main Pipeline ----------\n",
    "\n",
    "def process_policy(pdf_path, queries):\n",
    "    policy_results = []\n",
    "    print(f\"\\n📄 Processing {os.path.basename(pdf_path)}...\")\n",
    "    \n",
    "    text = extract_text(pdf_path)\n",
    "    chunks = chunk_policy_text(text)\n",
    "    chunk_vectors = normalize_vectors([embed(c) for c in tqdm(chunks, desc=\"Embedding Chunks\")])\n",
    "    \n",
    "    faiss_index = faiss.IndexFlatIP(embedding_dim)\n",
    "    faiss_index.add(chunk_vectors)\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\n🔍 Query: {query}\")\n",
    "        query_vec = normalize_vectors([embed(query)])\n",
    "        D, I = faiss_index.search(query_vec, 1)\n",
    "\n",
    "        top_idx = I[0][0]\n",
    "        top_chunk = chunks[top_idx]\n",
    "        score = float(D[0][0])\n",
    "        \n",
    "        summary = summarize_with_openrouter(top_chunk)\n",
    "        evaluation = evaluate_with_openrouter(query, top_chunk, summary)\n",
    "\n",
    "        policy_results.append({\n",
    "            \"query\": query,\n",
    "            \"retrieved_chunk\": top_chunk,\n",
    "            \"similarity_score\": score,\n",
    "            \"summary\": summary,\n",
    "            \"evaluation\": evaluation\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"policy_pdf\": os.path.basename(pdf_path),\n",
    "        \"results\": policy_results\n",
    "    }\n",
    "\n",
    "# ---------- Runner ----------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"D:\\NLPInsuranceProject\\NLPINSURANCE-FINTECHPROJ\\policy_pdfs\"\n",
    "    all_results = []\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            path = os.path.join(input_folder, filename)\n",
    "            result = process_policy(path, queries)\n",
    "            all_results.append(result)\n",
    "\n",
    "    with open(\"results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n✅ All policies processed. Results saved to `results.json`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac68b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
